{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Referrence\n",
    "- [Complete Tutorial on Vector Database - Learn ChromaDB, Pinecone & Weaviate | Generative AI](https://www.youtube.com/watch?v=8KrTO9bS91s)\n",
    "- https://github.com/entbappy/Complete-Generative-AI-Course-on-YouTube/blob/main/Vector%20Database/2.Pinecone_demo.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import All the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader, PyPDFLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openAi_key = os.getenv('OPENAI_API_KEY')\n",
    "pinecone_key = os.getenv('PINECONE_API_KEY')\n",
    "# openAi_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"  # Dummy key, actual is injected in the proxy\n",
    "# os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"OPENAI_API_BASE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Text from the PDF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_from_folder(folder_path: str) -> list[Document]:\n",
    "    all_docs: list[Document] = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.lower().endswith(\".pdf\"):\n",
    "            continue\n",
    "\n",
    "        full_path = os.path.join(folder_path, filename)\n",
    "        loader = PyMuPDFLoader(full_path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        # Derive title from filename, e.g. \"My Paper.pdf\" -> \"My Paper\"\n",
    "        title = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Add the title meta-field to each Document\n",
    "        for doc in docs:\n",
    "            # doc.metadata already contains things like 'source'; we just add 'Title'\n",
    "            doc.metadata[\"Title\"] = title\n",
    "\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    return all_docs\n",
    "\n",
    "\n",
    "# ✅ Load all PDFs in the folder\n",
    "data = load_pdfs_from_folder(\"website_content/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = PyPDFDirectoryLoader(\"website_content\")\n",
    "# # loader = PyPDFLoader(\"Website_Report_V1.pdf\")\n",
    "# data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_page(text: str) -> str:\n",
    "    # Remove zero-width space characters\n",
    "    text = text.replace('\\u200b', '')\n",
    "    \n",
    "    # Remove artificial newlines that break up sentences or words\n",
    "    text = re.sub(r'\\n+', ' ', text)             # Merge multiple newlines into one space\n",
    "    text = re.sub(r'(?<=\\w)-\\s+(?=\\w)', '', text) # Fix hyphenated line breaks (e.g., \"subsi-\\ndy\" → \"subsidy\")\n",
    "    text = re.sub(r'\\s+', ' ', text)             # Normalize extra spaces\n",
    "    return text.strip()\n",
    "\n",
    "def clean_documents(docs: list[Document]) -> list[Document]:\n",
    "    cleaned_docs = []\n",
    "    for doc in docs:\n",
    "        cleaned_text = clean_page(doc.page_content)\n",
    "        cleaned_docs.append(Document(page_content=cleaned_text, metadata=doc.metadata))\n",
    "    return cleaned_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_documents(data)\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Extracted Data into Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text_chunks))\n",
    "text_chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load OPENAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['OPENAI_API_KEY'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a specific embedding model\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"LangChain is an AI framework for LLMs.\"\n",
    "# vector = embeddings_model.embed_query(text)\n",
    "\n",
    "# print(len(vector))\n",
    "# print(vector[:5])  # Print first 5 values for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # os.environ['PINECONE_API_KEY'] = \"pcsk_7HmYTn_KS4n9fp4CzxTjTrKpYWaaBgHvP2JPRRx9fp5URALDkKuCC1yeZYhbZ557rEfjYT\"\n",
    "# # pc = pinecone.Pinecone(os.getenv('PINECONE_API_KEY'))\n",
    "# pc = pinecone.Pinecone(pinecone_key)\n",
    "index_name = \"fit5120-tm01\"\n",
    "# index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings for each of the Text Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pinecone.from_texts() vs Pinecone.from_documents()\n",
    "# # .from_documents() stores meta data while .from_texts() does not\n",
    "\n",
    "# docsearch = Pinecone.from_texts([t.page_content for t in text_chunks], embeddings_model, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you already have an index, you can load it like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Pinecone.from_existing_index(index_name, embeddings_model)\n",
    "docsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much is the diesel subisdy expenditure in 2024\"\n",
    "\n",
    "docs = docsearch.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a LLM Model Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_gpt4 = ChatOpenAI(model=\"gpt-4.1-nano\") #gpt-4.1-mini\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm_gpt4, chain_type=\"stuff\", retriever=docsearch.as_retriever(search_kwargs={\"k\": 3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much was the government expenditure?\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much is the diesel subisdy expenditure in 2024\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much is the diesel subsidy expenditure in 2024\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "  user_input = input(f\"Input Prompt: \")\n",
    "  if user_input == 'exit':\n",
    "    print('Exiting')\n",
    "    break\n",
    "  if user_input == '':\n",
    "    continue\n",
    "  result = qa.invoke({'query': user_input})\n",
    "  print(f\"Answer: {result['result']}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Replace this with your actual Cloudflare Worker URL\n",
    "PROXY_BASE_URL = \"https://policylensai.wanningc11.workers.dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_url = f\"{PROXY_BASE_URL}/v1/embeddings\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"text-embedding-3-small\",  # match what your backend expects\n",
    "    \"input\": \"Test embedding input from Jupyter\"\n",
    "}\n",
    "\n",
    "response = requests.post(embedding_url, headers=headers, json=payload)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response JSON:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_url = f\"{PROXY_BASE_URL}/v1/chat/completions\"\n",
    "\n",
    "chat_payload = {\n",
    "    \"model\": \"gpt-4.1-nano\",  # or gpt-4 if you're using that in backend\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ],\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "chat_response = requests.post(chat_url, headers=headers, json=chat_payload)\n",
    "\n",
    "print(\"Status Code:\", chat_response.status_code)\n",
    "print(\"Response JSON:\", chat_response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector (truncated): [0.008168850094079971, -0.00567884324118495, 0.03726189583539963, -0.003387290984392166, -0.004074247553944588]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # must match what you allow\n",
    ")\n",
    "\n",
    "result = embedding_model.embed_query(\"This is a test query from LangChain.\")\n",
    "print(\"Embedding vector (truncated):\", result[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",  # or \"gpt-4\" if you're using GPT-4\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "response = chat_model.invoke([\n",
    "    HumanMessage(content=\"What is the capital of Japan?\")\n",
    "])\n",
    "\n",
    "print(\"Response:\", response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Industry_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
